{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "import pyarrow.parquet as parquet\n",
    "import numpy as np\n",
    "# import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders= ['features/gt', 'features/test', 'features/sumb']\n",
    "# pds = [None, None, None]\n",
    "\n",
    "# num = 2\n",
    "# for i in range(num):\n",
    "#     pds[i] = parquet.read_table(folders[i], columns=['objectId']).to_pandas()\n",
    "# ids=set(pd.concat(pds).objectId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ids)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs=[]\n",
    "# for (dirpath, dirnames, filenames) in walk('../dataset/texts/train/'):\n",
    "#     for name in filenames:\n",
    "#         if name.startswith('part'):\n",
    "#             texts = pd.read_parquet('../dataset/texts/train/' + name,columns = ['objectId','lang','preprocessed'], engine='fastparquet')\n",
    "#             texts= texts[(texts.lang=='ru')]\n",
    "#             for i, line in texts[['objectId','preprocessed']].values:\n",
    "#                 docs.append(TaggedDocument([ i if type(i)==str else i.decode(\"utf-8\") for i in line],[str(i)]))\n",
    "\n",
    "test_texts = pd.read_parquet('../dataset/texts/test/',columns = ['objectId','lang','preprocessed'], engine='fastparquet')\n",
    "test_texts= test_texts[(test_texts.lang=='ru')]\n",
    "for i, line in test_texts[['objectId','preprocessed']].values:\n",
    "    docs.append(TaggedDocument([ i if type(i)==str else i.decode(\"utf-8\") for i in line],[str(i)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del test_texts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec( vector_size = 20 # Model initialization\n",
    "    , dm=0      \n",
    "    , window = 20\n",
    "    , min_count = 150\n",
    "    , workers = 24)\n",
    "\n",
    "model.build_vocab(docs) # Building vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_val = 0.065       # Initial learning rate\n",
    "min_alpha_val = 1e-4     # Minimum for linear learning rate decay\n",
    "passes = 4              # Number of passes of one document during training\n",
    "\n",
    "alpha_delta = (alpha_val - min_alpha_val) / (passes - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for epoch in range(passes):\n",
    "\n",
    "    # Shuffling gets better results\n",
    "\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    # Train\n",
    "\n",
    "    model.alpha, model.min_alpha = alpha_val, alpha_val\n",
    "\n",
    "\n",
    "\n",
    "    print('Start %i at alpha %f' % (epoch + 1, alpha_val))\n",
    "    \n",
    "    model.train(docs, epochs=3, total_examples=int(model.corpus_count))\n",
    "            \n",
    "    x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector(['кемеров','вишн','трагед','погибш','пожар','зимн'])], topn=5)])\n",
    "    for d in docs:\n",
    "        if int(d.tags[0]) in x1 :\n",
    "            print(' '.join(d.words[:10]))\n",
    "    \n",
    "    print('-------------')\n",
    "    x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector(['иисус','икон','бог','бож','свят'])], topn=5)])\n",
    "    for d in docs:\n",
    "        if int(d.tags[0]) in x1 :\n",
    "            print(' '.join(d.words[:10]))\n",
    "            \n",
    "    print('-------------')     \n",
    "    x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector(['приготовлен','сливочн','ингредиент','чеснок','репчат'])], topn=5)])\n",
    "    for d in docs:\n",
    "        if int(d.tags[0]) in x1 :\n",
    "            print(' '.join(d.words[:10]))\n",
    "\n",
    "    \n",
    "    \n",
    "#     x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector([\"сыночк\", \"маленьк\", \"ласков\"])], topn=5)])\n",
    "#     print([' '.join(_) for _ in all_[(all_.objectId.map(lambda x : x in x1))].preprocessed.values])\n",
    "#     x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector([\"ударн\", \"тренировк\", \"красот\"])], topn=5)])\n",
    "#     print([' '.join(_) for _ in all_[(all_.objectId.map(lambda x : x in x1))].preprocessed.values])\n",
    "#     x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector(['киев', 'торгов', 'развлекательн', 'центр', 'нарушен', 'пожарн', 'безопасн', 'будут', 'закрыва', 'суд', 'киевск', 'горадминистрац', 'намер', 'добива', 'суд', 'прекращен', 'работ', 'торгов', 'центр', 'котор', 'выявл', 'нарушен', 'норм', 'пожарн', 'безопасн', 'киевск', 'горадминистрац', 'намер', 'добива', 'суд', 'прекращен', 'работ', 'торгов', 'центр', 'котор', 'выявл', 'нарушен', 'норм', 'пожарн', 'безопасн', 'киев', 'торгов', 'развлекательн', 'центр', 'нарушен', 'пожарн', 'безопасн', 'будут', 'закрыва', 'суд'])], topn=5)])\n",
    "#     print([' '.join(_) for _ in all_[(all_.objectId.map(lambda x : x in x1))].preprocessed.values])\n",
    "\n",
    "    print('Completed pass %i at alpha %f' % (epoch + 1, alpha_val))\n",
    "\n",
    "    # Next run alpha\n",
    "\n",
    "    alpha_val -= alpha_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector(['кемеров','вишн','трагед','погибш','пожар','зимн'])], topn=5)])\n",
    "    for d in docs:\n",
    "        if int(d.tags[0]) in x1 :\n",
    "            print(' '.join(d.words[:10]))\n",
    "    \n",
    "    print('-------------')\n",
    "    x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector(['иисус','икон','бог','бож','свят'])], topn=5)])\n",
    "    for d in docs:\n",
    "        if int(d.tags[0]) in x1 :\n",
    "            print(' '.join(d.words[:10]))\n",
    "            \n",
    "    print('-------------')     \n",
    "    x1 = set([int(a[0]) for a in model.docvecs.most_similar(positive=[model.infer_vector(['приготовлен','сливочн','ингредиент','чеснок','репчат'])], topn=5)])\n",
    "    for d in docs:\n",
    "        if int(d.tags[0]) in x1 :\n",
    "            print(' '.join(d.words[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('doc2vec.csv', 'a') as f:\n",
    "    for (dirpath, dirnames, filenames) in walk('../dataset/texts/train/'):\n",
    "        for name in filenames:\n",
    "            if name.startswith('part'):\n",
    "                # Read single part\n",
    "                texts = pd.read_parquet('../dataset/texts/train/' + name,columns = ['objectId','preprocessed'], engine='fastparquet')        \n",
    "                texts['preprocessed']=texts.preprocessed.map(lambda x: [ i if type(i)==str else i.decode(\"utf-8\") for i in x])\n",
    "                texts['embedding'] = texts.preprocessed.apply(model.infer_vector)\n",
    "                for o, emb in texts[['objectId','embedding']].values:\n",
    "                    f.write(str(o) + ',' + ','.join([str(xxx) for xxx in emb]) + \"\\n\")\n",
    "\n",
    "    test_texts = pd.read_parquet('../dataset/texts/test/' ,columns = ['objectId','preprocessed'], engine='fastparquet')        \n",
    "    test_texts['preprocessed']=test_texts.preprocessed.map(lambda x: [ i if type(i)==str else i.decode(\"utf-8\") for i in x])\n",
    "    test_texts['embedding'] = test_texts.preprocessed.apply(model.infer_vector)\n",
    "    for o, emb in test_texts[['objectId','embedding']].values:\n",
    "        f.write(str(o) + ',' + ','.join([str(xxx) for xxx in emb]) + \"\\n\")\n",
    "#     parts.append(test_texts[['objectId','embedding']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = all_[['objectId','embedding']].set_index('objectId').to_dict()['embedding']\n",
    "np.save('doc2vecCache2', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_['len'] = all_.preprocessed.map(lambda x: len(x))\n",
    "d = all_[['objectId','len']].set_index('objectId').to_dict()['len']\n",
    "np.save('tokenslenCache2', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc2vec.csv', newline='') as f:\n",
    "    content = f.readlines()\n",
    "d = {}\n",
    "for x in content:\n",
    "    s= x.strip().split(\",\")\n",
    "    d[s[0]]=[float(xxx) for xxx in s[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('doc2vec', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load('doc2vec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[]\n",
    "K=[]\n",
    "for k,x in d.item().items():\n",
    "    X.append(x)\n",
    "    K.append(k)\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans,AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 200\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=30000, n_init=5)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=AgglomerativeClustering(n_clusters=70)\n",
    "model2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3721891,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.array(K),preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3721891,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdClasses = pd.DataFrame(preds, columns=['text_cluster_120'], index = K)\n",
    "pdClasses['instanceId_userId'] = pdClasses.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdClasses.to_hdf('text_cluster_120.h5', key='c', mode='w', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for k,v in zip(K, preds):\n",
    "    clusters[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('clusters120', clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= np.load('clusters120.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('clusters120.csv', 'a') as f:\n",
    "    for k,v in c.item().items():\n",
    "        f.write(str(k) + ','+  str(v) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = parquet.read_table('../texts/textsTest/', columns = ['objectId','preprocessed']).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters['517288']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster(28721520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = {k for k,v in clusters.items() if  v == 5}\n",
    "len(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts[(test_texts.objectId.map(lambda o : str(o) in cluster))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
